{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2213f7",
   "metadata": {},
   "source": [
    "# Practica 3\n",
    "Nombre: Ulises Abdiel Cabello Cardenas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f716d72",
   "metadata": {},
   "source": [
    "Para esta practica primero se importa lo que se va a ocupar:\n",
    "- Torch: que permite trabajar con tensores.\n",
    "- torch.nn: que tiene las herramientas para las redes neuronales como las funciones de activacion y de perdida\n",
    "- torch.optim: incluye los optimizadores\n",
    "- Dataset y dataloader: carga los datos por lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f71c3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee094c",
   "metadata": {},
   "source": [
    "## Carga del dataset\n",
    "Primero creamos el dataset de la tabla dada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f86dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Automata_dta(Dataset):\n",
    "    def __init__(self):\n",
    "        self.X = torch.tensor([\n",
    "            [0, 0, 1],  \n",
    "            [0, 0, 0],  \n",
    "            [0, 1, 1],  \n",
    "            [0, 1, 0],  \n",
    "            [1, 0, 1],  \n",
    "            [1, 0, 0],  \n",
    "            [1, 1, 1],  \n",
    "            [1, 1, 0],  \n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        self.Y = torch.tensor([\n",
    "            [0, 0],  \n",
    "            [0, 1],  \n",
    "            [0, 0],  \n",
    "            [1, 0],  \n",
    "            [1, 0],  \n",
    "            [1, 1],  \n",
    "            [1, 0],  \n",
    "            [0, 0],  \n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b04e17",
   "metadata": {},
   "source": [
    "## Carga de datos y separacion\n",
    "En esta parte cargamos los datos y lo separamos por lotes, se escoje separar los datos por lotes de 4 y mezclamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e470af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Automata_dta()\n",
    "entrenamiento = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b2937",
   "metadata": {},
   "source": [
    "## Creacion de la red neuronal\n",
    "Creamos una red neuronal para le automata, para esto tomamos 3 capas de entrada y 5 ocultas que van a pasar a las de salida que son 2, en este caso se toma la funcion sigmoide que es parecida a la funcion logistica solo que esta empieza en 0, se toma esta funcion de activacion ya que es la solicitada y es muy buena cuando se requieren valores binarios en la salida, aunque tenga inconvenientes como el desvanecimiento de gradiente, ya que es muy buena para valores muy cercanos al cero, pero cuando se aleja la derivada es muy peque;a y cuando se hace el backgroud la red aprendera poco.\n",
    "\n",
    "Para la funcion forward los datos fluiran en la red neuronal hacia adelante con la funcion sigmoide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82e59a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 5)  # Capa oculta de 5 neuronas\n",
    "        self.sigmoid = nn.Sigmoid() # Función de activación\n",
    "        self.fc2 = nn.Linear(5, 2)  # Capa de salida de 2 neuronas\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))  # Capa oculta + activación Sigmoide\n",
    "        x = self.sigmoid(self.fc2(x))  # Capa de salida + activación Sigmoide\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f4967",
   "metadata": {},
   "source": [
    "## Inicializacion\n",
    "En esta parte del codigo se instancia la red y se definel la funcion de perdida que ayda a determinar que tan bien esta prediciendo la red neuronal, como optimizador de la red se toma el SGD que es el gradiente descendiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70a6751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN()\n",
    "criterio = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f20d37",
   "metadata": {},
   "source": [
    "## Entrenamos el modelo\n",
    "Para ello tomamos 10000 iteraciones que se van a hacer al modelo, primero inicializamos la perdida en cero e iteramos sobre los lotes que se partieron, despues reiniciamos los gradientes y en `outputs = model(inputs)` calculamos las predicciones de la red neuronal, despues vemos y se compara los valores de el anterior con los valores reales aqui se ocupa el BCE de perdida, en `loss.backward()` se calculan los grdientes y es la fase donde se modifican los pesos posteriormente se actualizan y se cambia el total de perdida.\n",
    "\n",
    "Al ultimo solo se muestra la perdida de la ultima iteracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "950db4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época [10000/10000], Pérdida: 0.0495\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000  \n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in entrenamiento:\n",
    "        optimizer.zero_grad()  # Reiniciamos gradientes\n",
    "        outputs = model(inputs)  # Forward propagation\n",
    "        loss = criterio(outputs, labels)  # Cálculo de pérdida\n",
    "        loss.backward()  # Backward propagation\n",
    "        optimizer.step()  # Actualización de pesos\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10000 == 0:\n",
    "        print(f'Época [{epoch+1}/{epochs}], Pérdida: {total_loss/len(entrenamiento):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1d15a",
   "metadata": {},
   "source": [
    "## Evaluacion del modelo\n",
    "\n",
    "En esta parte se evalua el modelo y se redondean los valores con round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd31229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicciones finales:\n",
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Evaluación del modelo\n",
    "    test_outputs = model(dataset.X)\n",
    "    \n",
    "    #  Convertir valores a 0 o 1 usando `round()`\n",
    "    predicted = test_outputs.round()\n",
    "    \n",
    "    print(\"\\nPredicciones finales:\")\n",
    "    print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
